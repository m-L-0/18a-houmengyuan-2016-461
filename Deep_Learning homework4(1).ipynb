{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业\n",
    "\n",
    "1. 浮现上述代码。\n",
    "2. 计算模型参数。\n",
    "3. 使用不同大小的批次进行实验，观察模型收敛速度与收敛平稳性。\n",
    "4. 使用不同大小的学习率进行实验，观察模型收敛速度与收敛平稳性。\n",
    "5. 使用不同的激活函数进行实验，观察模型收敛速度与收敛平性能。\n",
    "6. 尝试使用不同的参数初始化方法（如使用不同的正态分布、均匀分布、固定值等方法），观察模型收敛速度。\n",
    "7. 思考如何改进模型以使得模型性能增强。\n",
    "8. 思考如何给模型添加新的隐藏层并进行实验。\n",
    "\n",
    "*说明：需对上述问题进行代码实现与结论总结*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.代码实现（MNIST分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\envs\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 下载MNIST数据集并生成DataSet对象\n",
    "# 使用OneHot编码处理标记\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1de162db1d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 训练集图片矩阵，代表55000张图片，每张图片为一个向量，其长度为784\n",
    "mnist.train.images.shape\n",
    "# 训练集标记矩阵，代表55000张图片的标记，每张图片为一个10维的独热编码向量\n",
    "mnist.train.labels.shape\n",
    "# 可视化训练集中的图片\n",
    "plt.imshow(Image.fromarray(mnist.train.images[0].reshape(28, 28)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 14.4170, acc 0.0895\n",
      "step   500, loss 13.0831, acc 0.2278\n",
      "step  1000, loss 11.0823, acc 0.3058\n",
      "step  1500, loss 9.5701, acc 0.3501\n",
      "step  2000, loss 10.0569, acc 0.3685\n",
      "step  2500, loss 9.5701, acc 0.3918\n",
      "step  3000, loss 10.4833, acc 0.4185\n",
      "step  3500, loss 8.5657, acc 0.4311\n",
      "step  4000, loss 7.1814, acc 0.4433\n",
      "step  4500, loss 10.0564, acc 0.4494\n",
      "step  5000, loss 10.5775, acc 0.4524\n",
      "step  5500, loss 8.0534, acc 0.4578\n",
      "step  6000, loss 8.0571, acc 0.4604\n",
      "step  6500, loss 8.5627, acc 0.4604\n",
      "step  7000, loss 9.0666, acc 0.4645\n",
      "step  7500, loss 7.5554, acc 0.4651\n",
      "step  8000, loss 8.5627, acc 0.4677\n",
      "step  8500, loss 8.6460, acc 0.4654\n",
      "step  9000, loss 8.0586, acc 0.4709\n",
      "step  9500, loss 6.5480, acc 0.4682\n",
      "step 10000, loss 11.0179, acc 0.4707\n",
      "step 10500, loss 9.3627, acc 0.4707\n",
      "step 11000, loss 5.0543, acc 0.4710\n",
      "step 11500, loss 9.6109, acc 0.4717\n",
      "step 12000, loss 6.6403, acc 0.4864\n",
      "step 12500, loss 6.9172, acc 0.5315\n",
      "step 13000, loss 9.0499, acc 0.5386\n",
      "step 13500, loss 6.3888, acc 0.5483\n",
      "step 14000, loss 7.8844, acc 0.5555\n",
      "step 14500, loss 5.5402, acc 0.5545\n",
      "step 15000, loss 8.6387, acc 0.5613\n",
      "step 15500, loss 6.4022, acc 0.5607\n",
      "step 16000, loss 9.0664, acc 0.5641\n",
      "step 16500, loss 5.5406, acc 0.5630\n",
      "step 17000, loss 7.5554, acc 0.5644\n",
      "step 17500, loss 6.0443, acc 0.5621\n",
      "step 18000, loss 7.5557, acc 0.5685\n",
      "step 18500, loss 5.2154, acc 0.5668\n",
      "step 19000, loss 5.8045, acc 0.5669\n",
      "step 19500, loss 7.5554, acc 0.5664\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.relu(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(\n",
    "        labels * tf.log(output + 1e-7),\n",
    "        axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 使用不同大小的批次进行实验，观察模型收敛速度与收敛平稳性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 15.1106, acc 0.0741\n",
      "step   500, loss 10.1695, acc 0.3694\n",
      "step  1000, loss 5.6972, acc 0.4635\n",
      "step  1500, loss 9.0665, acc 0.5054\n",
      "step  2000, loss 6.7838, acc 0.5215\n",
      "step  2500, loss 5.5212, acc 0.5431\n",
      "step  3000, loss 4.2321, acc 0.5707\n",
      "step  3500, loss 6.1932, acc 0.5945\n",
      "step  4000, loss 8.0601, acc 0.6119\n",
      "step  4500, loss 4.1117, acc 0.6170\n",
      "step  5000, loss 6.0443, acc 0.6236\n",
      "step  5500, loss 2.2253, acc 0.6265\n",
      "step  6000, loss 10.6149, acc 0.6399\n",
      "step  6500, loss 5.5426, acc 0.6359\n",
      "step  7000, loss 4.8485, acc 0.6372\n",
      "step  7500, loss 6.5473, acc 0.6430\n",
      "step  8000, loss 6.5476, acc 0.6439\n",
      "step  8500, loss 7.3296, acc 0.6456\n",
      "step  9000, loss 4.5332, acc 0.6471\n",
      "step  9500, loss 5.5421, acc 0.6448\n",
      "step 10000, loss 4.9080, acc 0.6473\n",
      "step 10500, loss 5.5778, acc 0.6530\n",
      "step 11000, loss 5.4058, acc 0.6493\n",
      "step 11500, loss 4.0317, acc 0.6541\n",
      "step 12000, loss 4.4969, acc 0.6555\n",
      "step 12500, loss 5.0406, acc 0.6534\n",
      "step 13000, loss 4.3453, acc 0.6512\n",
      "step 13500, loss 4.5335, acc 0.6571\n",
      "step 14000, loss 6.4981, acc 0.6578\n",
      "step 14500, loss 2.8915, acc 0.6544\n",
      "step 15000, loss 4.9439, acc 0.6576\n",
      "step 15500, loss 4.5327, acc 0.6574\n",
      "step 16000, loss 6.6400, acc 0.6603\n",
      "step 16500, loss 4.0295, acc 0.6598\n",
      "step 17000, loss 4.5332, acc 0.6600\n",
      "step 17500, loss 6.8840, acc 0.6609\n",
      "step 18000, loss 8.0590, acc 0.6594\n",
      "step 18500, loss 5.0369, acc 0.6597\n",
      "step 19000, loss 5.7830, acc 0.6638\n",
      "step 19500, loss 6.0443, acc 0.6601\n"
     ]
    }
   ],
   "source": [
    "#batch_size为32\n",
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.relu(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(\n",
    "        labels * tf.log(output + 1e-7),\n",
    "        axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 14.2668, acc 0.1293\n",
      "step   500, loss 11.9702, acc 0.2982\n",
      "step  1000, loss 10.5203, acc 0.3733\n",
      "step  1500, loss 10.5853, acc 0.4206\n",
      "step  2000, loss 7.7107, acc 0.4420\n",
      "step  2500, loss 8.8561, acc 0.4577\n",
      "step  3000, loss 9.1410, acc 0.4911\n",
      "step  3500, loss 7.9095, acc 0.5136\n",
      "step  4000, loss 8.0744, acc 0.5273\n",
      "step  4500, loss 7.5194, acc 0.5588\n",
      "step  5000, loss 6.6604, acc 0.6046\n",
      "step  5500, loss 7.3150, acc 0.6210\n",
      "step  6000, loss 7.1834, acc 0.6229\n",
      "step  6500, loss 5.4105, acc 0.6344\n",
      "step  7000, loss 6.6820, acc 0.6366\n",
      "step  7500, loss 6.7337, acc 0.6535\n",
      "step  8000, loss 4.7851, acc 0.6891\n",
      "step  8500, loss 3.7563, acc 0.6996\n",
      "step  9000, loss 4.4105, acc 0.7186\n",
      "step  9500, loss 3.7313, acc 0.7231\n",
      "step 10000, loss 4.6536, acc 0.7285\n",
      "step 10500, loss 3.3384, acc 0.7377\n",
      "step 11000, loss 3.4381, acc 0.7780\n",
      "step 11500, loss 1.7726, acc 0.7915\n",
      "step 12000, loss 2.0170, acc 0.8069\n",
      "step 12500, loss 2.5233, acc 0.8114\n",
      "step 13000, loss 1.8826, acc 0.8160\n",
      "step 13500, loss 2.8268, acc 0.8209\n",
      "step 14000, loss 3.8486, acc 0.8236\n",
      "step 14500, loss 3.2330, acc 0.8273\n",
      "step 15000, loss 3.0222, acc 0.8224\n",
      "step 15500, loss 2.6992, acc 0.8307\n",
      "step 16000, loss 2.7446, acc 0.8300\n",
      "step 16500, loss 2.4613, acc 0.8297\n",
      "step 17000, loss 2.1125, acc 0.8311\n",
      "step 17500, loss 2.6042, acc 0.8294\n",
      "step 18000, loss 1.7482, acc 0.8367\n",
      "step 18500, loss 3.0201, acc 0.8309\n",
      "step 19000, loss 2.3210, acc 0.8341\n",
      "step 19500, loss 2.5121, acc 0.8373\n"
     ]
    }
   ],
   "source": [
    "#batch_size为64\n",
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.relu(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(\n",
    "        labels * tf.log(output + 1e-7),\n",
    "        axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(64)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 64):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(64)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.结论：当batch_size从32变为64，则收敛速度加快，并且最终正确率得到提升"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 使用不同大小的学习率进行实验，观察模型收敛速度与收敛平稳性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 13.9661, acc 0.1054\n",
      "step   500, loss 11.8590, acc 0.3041\n",
      "step  1000, loss 9.3819, acc 0.4006\n",
      "step  1500, loss 6.5480, acc 0.4610\n",
      "step  2000, loss 7.3277, acc 0.4999\n",
      "step  2500, loss 8.8116, acc 0.5043\n",
      "step  3000, loss 8.7321, acc 0.5187\n",
      "step  3500, loss 7.2510, acc 0.5256\n",
      "step  4000, loss 7.0707, acc 0.5240\n",
      "step  4500, loss 8.5539, acc 0.5407\n",
      "step  5000, loss 7.5545, acc 0.5365\n",
      "step  5500, loss 9.5701, acc 0.5440\n",
      "step  6000, loss 7.5552, acc 0.5402\n",
      "step  6500, loss 7.2045, acc 0.5450\n",
      "step  7000, loss 6.0833, acc 0.5487\n",
      "step  7500, loss 6.3333, acc 0.5460\n",
      "step  8000, loss 9.3541, acc 0.5468\n",
      "step  8500, loss 6.3944, acc 0.5486\n",
      "step  9000, loss 7.5577, acc 0.5523\n",
      "step  9500, loss 6.7639, acc 0.5522\n",
      "step 10000, loss 10.3935, acc 0.5542\n",
      "step 10500, loss 6.5524, acc 0.5529\n",
      "step 11000, loss 6.5481, acc 0.5583\n",
      "step 11500, loss 8.8428, acc 0.5535\n",
      "step 12000, loss 5.0446, acc 0.5588\n",
      "step 12500, loss 7.0517, acc 0.5578\n",
      "step 13000, loss 10.0738, acc 0.5545\n",
      "step 13500, loss 7.0517, acc 0.5599\n",
      "step 14000, loss 4.5332, acc 0.5562\n",
      "step 14500, loss 8.0590, acc 0.5650\n",
      "step 15000, loss 6.5480, acc 0.5549\n",
      "step 15500, loss 7.3979, acc 0.5641\n",
      "step 16000, loss 7.5554, acc 0.5593\n",
      "step 16500, loss 7.0517, acc 0.5671\n",
      "step 17000, loss 7.2041, acc 0.5600\n",
      "step 17500, loss 7.0517, acc 0.5583\n",
      "step 18000, loss 7.0517, acc 0.5653\n",
      "step 18500, loss 6.8067, acc 0.5632\n",
      "step 19000, loss 6.5480, acc 0.5650\n",
      "step 19500, loss 6.5480, acc 0.5627\n"
     ]
    }
   ],
   "source": [
    "#学习率为0.01\n",
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.relu(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(\n",
    "        labels * tf.log(output + 1e-7),\n",
    "        axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 12.1040, acc 0.1081\n",
      "step   500, loss 7.0517, acc 0.6029\n",
      "step  1000, loss 7.0544, acc 0.6320\n",
      "step  1500, loss 4.5154, acc 0.6529\n",
      "step  2000, loss 4.0294, acc 0.6661\n",
      "step  2500, loss 3.0221, acc 0.7278\n",
      "step  3000, loss 5.0369, acc 0.7292\n",
      "step  3500, loss 2.8108, acc 0.7396\n",
      "step  4000, loss 3.1531, acc 0.7330\n",
      "step  4500, loss 4.1733, acc 0.7469\n",
      "step  5000, loss 4.0296, acc 0.7394\n",
      "step  5500, loss 3.3815, acc 0.7449\n",
      "step  6000, loss 3.0237, acc 0.7453\n",
      "step  6500, loss 4.0295, acc 0.7530\n",
      "step  7000, loss 2.5185, acc 0.7513\n",
      "step  7500, loss 2.5185, acc 0.7546\n",
      "step  8000, loss 5.0369, acc 0.7492\n",
      "step  8500, loss 2.5185, acc 0.7537\n",
      "step  9000, loss 5.5414, acc 0.7599\n",
      "step  9500, loss 3.5258, acc 0.7576\n",
      "step 10000, loss 1.5111, acc 0.7594\n",
      "step 10500, loss 5.5462, acc 0.7545\n",
      "step 11000, loss 5.8735, acc 0.7576\n",
      "step 11500, loss 1.5111, acc 0.7621\n",
      "step 12000, loss 3.1125, acc 0.7570\n",
      "step 12500, loss 2.5079, acc 0.8460\n",
      "step 13000, loss 0.2851, acc 0.8523\n",
      "step 13500, loss 1.5111, acc 0.8548\n",
      "step 14000, loss 1.6627, acc 0.8517\n",
      "step 14500, loss 1.0074, acc 0.8546\n",
      "step 15000, loss 2.6923, acc 0.8538\n",
      "step 15500, loss 0.5228, acc 0.8603\n",
      "step 16000, loss 1.0074, acc 0.8581\n",
      "step 16500, loss 1.5111, acc 0.8563\n",
      "step 17000, loss 2.7342, acc 0.8601\n",
      "step 17500, loss 0.5058, acc 0.8593\n",
      "step 18000, loss 2.6882, acc 0.8599\n",
      "step 18500, loss 2.0138, acc 0.8623\n",
      "step 19000, loss 1.0074, acc 0.8615\n",
      "step 19500, loss 2.2338, acc 0.8627\n"
     ]
    }
   ],
   "source": [
    "#学习率为0.1\n",
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.relu(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(\n",
    "        labels * tf.log(output + 1e-7),\n",
    "        axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.结论：使用了0.01和0.1的学习率来训练模型，\n",
    "学习率为0.1的收敛速度明显提升，但收敛平稳性略显下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 使用不同的激活函数进行实验，观察模型收敛速度与收敛平性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 14.6317, acc 0.1046\n",
      "step   500, loss 10.4235, acc 0.3019\n",
      "step  1000, loss 11.7152, acc 0.3626\n",
      "step  1500, loss 10.0738, acc 0.4872\n",
      "step  2000, loss 7.0513, acc 0.5648\n",
      "step  2500, loss 7.8593, acc 0.6031\n",
      "step  3000, loss 4.2247, acc 0.6453\n",
      "step  3500, loss 4.8906, acc 0.6852\n",
      "step  4000, loss 5.0369, acc 0.6949\n",
      "step  4500, loss 4.9454, acc 0.7067\n",
      "step  5000, loss 3.6370, acc 0.7072\n",
      "step  5500, loss 5.0411, acc 0.7220\n",
      "step  6000, loss 3.5254, acc 0.7232\n",
      "step  6500, loss 4.5332, acc 0.7267\n",
      "step  7000, loss 5.3793, acc 0.7386\n",
      "step  7500, loss 3.0707, acc 0.7394\n",
      "step  8000, loss 2.5424, acc 0.7543\n",
      "step  8500, loss 3.7666, acc 0.7689\n",
      "step  9000, loss 2.5706, acc 0.7780\n",
      "step  9500, loss 4.4105, acc 0.7907\n",
      "step 10000, loss 2.3650, acc 0.7840\n",
      "step 10500, loss 4.5298, acc 0.7945\n",
      "step 11000, loss 2.4895, acc 0.7988\n",
      "step 11500, loss 1.0112, acc 0.8040\n",
      "step 12000, loss 1.5270, acc 0.8058\n",
      "step 12500, loss 3.3103, acc 0.8103\n",
      "step 13000, loss 2.6404, acc 0.8110\n",
      "step 13500, loss 3.4979, acc 0.8151\n",
      "step 14000, loss 3.2348, acc 0.8113\n",
      "step 14500, loss 1.5141, acc 0.8162\n",
      "step 15000, loss 3.6993, acc 0.8180\n",
      "step 15500, loss 3.5258, acc 0.8153\n",
      "step 16000, loss 3.0441, acc 0.8212\n",
      "step 16500, loss 3.3447, acc 0.8243\n",
      "step 17000, loss 2.1153, acc 0.8164\n",
      "step 17500, loss 2.5185, acc 0.8229\n",
      "step 18000, loss 1.8113, acc 0.8165\n",
      "step 18500, loss 2.1003, acc 0.8257\n",
      "step 19000, loss 2.0646, acc 0.8248\n",
      "step 19500, loss 3.5274, acc 0.8225\n"
     ]
    }
   ],
   "source": [
    "#激活函数为relu\n",
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.relu(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(\n",
    "        labels * tf.log(output + 1e-7),\n",
    "        axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 13.1870, acc 0.0816\n",
      "step   500, loss 7.3429, acc 0.3044\n",
      "step  1000, loss 3.8472, acc 0.4721\n",
      "step  1500, loss 2.3034, acc 0.5868\n",
      "step  2000, loss 3.0390, acc 0.6491\n",
      "step  2500, loss 1.8143, acc 0.6841\n",
      "step  3000, loss 2.5013, acc 0.7071\n",
      "step  3500, loss 0.6426, acc 0.7300\n",
      "step  4000, loss 1.5636, acc 0.7415\n",
      "step  4500, loss 2.6909, acc 0.7621\n",
      "step  5000, loss 0.9137, acc 0.7625\n",
      "step  5500, loss 2.3269, acc 0.7802\n",
      "step  6000, loss 0.8176, acc 0.7855\n",
      "step  6500, loss 2.2837, acc 0.7861\n",
      "step  7000, loss 0.6108, acc 0.7965\n",
      "step  7500, loss 1.0500, acc 0.8040\n",
      "step  8000, loss 0.5499, acc 0.8015\n",
      "step  8500, loss 1.2555, acc 0.8068\n",
      "step  9000, loss 0.6654, acc 0.8088\n",
      "step  9500, loss 0.2706, acc 0.8185\n",
      "step 10000, loss 0.0811, acc 0.8217\n",
      "step 10500, loss 1.1363, acc 0.8175\n",
      "step 11000, loss 0.9602, acc 0.8237\n",
      "step 11500, loss 1.2236, acc 0.8253\n",
      "step 12000, loss 0.9316, acc 0.8329\n",
      "step 12500, loss 0.4959, acc 0.8309\n",
      "step 13000, loss 0.9986, acc 0.8306\n",
      "step 13500, loss 0.4243, acc 0.8410\n",
      "step 14000, loss 0.9191, acc 0.8339\n",
      "step 14500, loss 1.0160, acc 0.8398\n",
      "step 15000, loss 0.6683, acc 0.8455\n",
      "step 15500, loss 1.2459, acc 0.8458\n",
      "step 16000, loss 0.5464, acc 0.8435\n",
      "step 16500, loss 0.9003, acc 0.8487\n",
      "step 17000, loss 1.2357, acc 0.8499\n",
      "step 17500, loss 0.8946, acc 0.8435\n",
      "step 18000, loss 0.8052, acc 0.8499\n",
      "step 18500, loss 1.4755, acc 0.8542\n",
      "step 19000, loss 0.5857, acc 0.8503\n",
      "step 19500, loss 1.1214, acc 0.8588\n"
     ]
    }
   ],
   "source": [
    "#激活函数为tf.nn.tanh\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.tanh(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(\n",
    "        labels * tf.log(output + 1e-7),\n",
    "        axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.结论：使用relu和tanh激活函数，\n",
    "tanh函数的正确率较高、收敛速度较快，两者收敛平稳性较为相似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 13.4218, acc 0.0660\n",
      "step   500, loss 8.9105, acc 0.3186\n",
      "step  1000, loss 3.3918, acc 0.5064\n",
      "step  1500, loss 2.4479, acc 0.6063\n",
      "step  2000, loss 4.0888, acc 0.6577\n",
      "step  2500, loss 1.9058, acc 0.7046\n",
      "step  3000, loss 1.9316, acc 0.7185\n",
      "step  3500, loss 2.2577, acc 0.7472\n",
      "step  4000, loss 2.2516, acc 0.7611\n",
      "step  4500, loss 1.9170, acc 0.7698\n",
      "step  5000, loss 2.2147, acc 0.7787\n",
      "step  5500, loss 0.9159, acc 0.7905\n",
      "step  6000, loss 1.1494, acc 0.7893\n",
      "step  6500, loss 2.0744, acc 0.7968\n",
      "step  7000, loss 1.2171, acc 0.8159\n",
      "step  7500, loss 1.0581, acc 0.8061\n",
      "step  8000, loss 1.7011, acc 0.8181\n",
      "step  8500, loss 1.1535, acc 0.8192\n",
      "step  9000, loss 1.1348, acc 0.8213\n",
      "step  9500, loss 0.8890, acc 0.8264\n",
      "step 10000, loss 0.5943, acc 0.8276\n",
      "step 10500, loss 0.1172, acc 0.8303\n",
      "step 11000, loss 1.1813, acc 0.8361\n",
      "step 11500, loss 1.2585, acc 0.8348\n",
      "step 12000, loss 1.7443, acc 0.8400\n",
      "step 12500, loss 0.0216, acc 0.8379\n",
      "step 13000, loss 1.6406, acc 0.8421\n",
      "step 13500, loss 1.5163, acc 0.8471\n",
      "step 14000, loss 0.6295, acc 0.8439\n",
      "step 14500, loss 1.1137, acc 0.8494\n",
      "step 15000, loss 0.6910, acc 0.8518\n",
      "step 15500, loss 1.3457, acc 0.8557\n",
      "step 16000, loss 0.6215, acc 0.8503\n",
      "step 16500, loss 0.0868, acc 0.8510\n",
      "step 17000, loss 0.4038, acc 0.8605\n",
      "step 17500, loss 2.0498, acc 0.8567\n",
      "step 18000, loss 0.3293, acc 0.8609\n",
      "step 18500, loss 0.8048, acc 0.8629\n",
      "step 19000, loss 0.2565, acc 0.8625\n",
      "step 19500, loss 0.5187, acc 0.8587\n"
     ]
    }
   ],
   "source": [
    "#初始化方法：tf.random_normal\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.tanh(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(\n",
    "        labels * tf.log(output + 1e-7),\n",
    "        axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 5.7949, acc 0.0988\n",
      "step   500, loss 0.9616, acc 0.5694\n",
      "step  1000, loss 0.8580, acc 0.6836\n",
      "step  1500, loss 0.8155, acc 0.7275\n",
      "step  2000, loss 0.5358, acc 0.7626\n",
      "step  2500, loss 0.5651, acc 0.7824\n",
      "step  3000, loss 0.6623, acc 0.7970\n",
      "step  3500, loss 0.3869, acc 0.8071\n",
      "step  4000, loss 0.2336, acc 0.8208\n",
      "step  4500, loss 0.7068, acc 0.8191\n",
      "step  5000, loss 0.6389, acc 0.8344\n",
      "step  5500, loss 0.7569, acc 0.8374\n",
      "step  6000, loss 0.3758, acc 0.8423\n",
      "step  6500, loss 0.7103, acc 0.8404\n",
      "step  7000, loss 0.6423, acc 0.8555\n",
      "step  7500, loss 0.5172, acc 0.8464\n",
      "step  8000, loss 0.7198, acc 0.8603\n",
      "step  8500, loss 0.2785, acc 0.8541\n",
      "step  9000, loss 0.6217, acc 0.8638\n",
      "step  9500, loss 0.4634, acc 0.8579\n",
      "step 10000, loss 0.4097, acc 0.8647\n",
      "step 10500, loss 0.5834, acc 0.8635\n",
      "step 11000, loss 0.7521, acc 0.8649\n",
      "step 11500, loss 0.4980, acc 0.8693\n",
      "step 12000, loss 0.5040, acc 0.8658\n",
      "step 12500, loss 0.2518, acc 0.8746\n",
      "step 13000, loss 0.2739, acc 0.8706\n",
      "step 13500, loss 0.5052, acc 0.8690\n",
      "step 14000, loss 0.3092, acc 0.8730\n",
      "step 14500, loss 0.7105, acc 0.8740\n",
      "step 15000, loss 0.2544, acc 0.8722\n",
      "step 15500, loss 0.5640, acc 0.8682\n",
      "step 16000, loss 0.3985, acc 0.8754\n",
      "step 16500, loss 0.1592, acc 0.8788\n",
      "step 17000, loss 0.2718, acc 0.8763\n",
      "step 17500, loss 0.4289, acc 0.8760\n",
      "step 18000, loss 0.3582, acc 0.8773\n",
      "step 18500, loss 0.3847, acc 0.8778\n",
      "step 19000, loss 0.2369, acc 0.8743\n",
      "step 19500, loss 0.5476, acc 0.8777\n"
     ]
    }
   ],
   "source": [
    "#初始化方法：tf.random_uniform\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.tanh(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_uniform([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(\n",
    "        labels * tf.log(output + 1e-7),\n",
    "        axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.使用tf.random_normal和tf.random_uniform两种不同的初始化方法\n",
    "使用tf.random_uniform比tf.random_normal的正确率高，且收敛效果更平稳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7. 思考如何改进模型以使得模型性能增强。\n",
    "可以变化批次，学习率，激活函数，初始化方法，改变不同的组合方式，\n",
    "找到使模型性能达到最优的组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8. 思考如何给模型添加新的隐藏层并进行实验。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
